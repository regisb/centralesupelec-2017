<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="description" content="Heroes &amp; Villains of AI">
        <meta name="author" content="Régis Behmo">
        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

        <title>Calculs distribués avec Apache Spark | CentraleSupelec, 6 novembre 2017</title>
        <link rel="stylesheet" href="static/css/reveal.css">
        <link rel="stylesheet" href="static/css/theme/night.css">
        <link rel="stylesheet" href="static/css/font-awesome.min.css">
        <link rel="stylesheet" href="static/css/highlightjs/monokai.css">

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'static/css/print/pdf.css' : 'static/css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>
        <style>
            .orange, h1.orange, h2.orange, h3.orange {
                color: #e7ad52;
            }
            .reveal table th, .reveal table td {
                text-align: center;
            }
            .reveal table th:not(:last-child), .reveal table td:not(:last-child) {
                border-right: 1px solid white
            }
            .reveal table tr.fragment:not(.visible) td {
                border: none;
            }
        </style>
    </head>

    <body>
        <div class="reveal">
            <div class="slides">
                <section>
                    <h2>Calculs Distribués avec Apache Spark</h2>
                    <p><a href="https://minutebutterfly.com">Régis Behmo</a><br>
                    <a href="https://github.com/regisb"><i class="fa fa-github"></i></a>
                    <a href="https://stackoverflow.com/users/356528/r%c3%a9gis-b?tab=profile"><i class="fa fa-stack-overflow"></i></a>
                    regisb<br></p>
                    <p>CentraleSupelec, 6 novembre 2017</p>
                    <p>Slides: <a href="https://regisb.github.io/centralesupelec-2017">https://regisb.github.io/centralesupelec-2017</a></p>
                </section>

                <section></section>

                <section>
                    <h1>Apache Spark</h1>
                    <p>Slides: <a href="https://regisb.github.io/centralesupelec-2017">https://regisb.github.io/centralesupelec-2017</a></p>
                </section>

                <section>
                    <table>
                        <tr class="orange">
                            <th>Hadoop</th>
                            <th>Spark</th>
                        </tr>
                        <tr class="fragment">
                            <td colspan="2">Java Virtual Machine (JVM)</td>
                        </tr>
                        <tr class="fragment">
                            <td>Write to disk (HDFS)</td>
                            <td>In-memory</td>
                        </tr>
                        <tr class="fragment">
                            <td>Native data structures</td>
                            <td>Resilient Distributed Datasets (RDD)</td>
                        </tr>
                        <tr class="fragment">
                            <td>Java (+ Hadoop streaming)</td>
                            <td>Java + Scala + <span class="orange">Python</span> + R</td>
                        </tr>
                        <tr class="fragment">
                            <td>-</td>
                            <td><span class="orange">Python</span> + Scala shell</td>
                        </tr>
                        <tr class="fragment">
                            <td>Pluggable SQL (Hive)</td>
                            <td>Spark SQL (native)</td>
                        </tr>
                        <tr class="fragment">
                            <td>Pluggable ML</td>
                            <td>Spark ML (native)</td>
                        </tr>
                    </table>
                </section>

                <section>
                    <h3>Installation</h3>
                    <ul>
                        <li>Java Runtime Environment (JRE)
                            <pre><code class="shell">$ sudo apt-get install default-jre
$ java -version
openjdk version "1.8.0_131"
OpenJDK Runtime Environment (build 1.8.0_131-8u131-b11-2ubuntu1.16.04.3-b11)
OpenJDK 64-Bit Server VM (build 25.131-b11, mixed mode)</code></pre>
                        </li>
                        <li>Python 3
                            <pre><code class="shell">$ sudo apt-get install python3</code></pre>
                        </li>
                        <li>Spark download (2.2.0 pre-built for Hadoop 2.7)
                            <pre><code class="shell">$ wget http://apache.crihan.fr/dist/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.7.tgz
$ tar xzf spark-2.2.0-bin-hadoop2.7.tgz</code></pre>
                        </li>
                    </ul>
                </section>

                <section>
                    <h3>Installation (from VM)</h3>
                    <ul>
                        <li>Go to Google Drive/Spark</li>
                        <li>Download bigbox.7z (7.1 Gb)</li>
                        <li>Uncompress with 7zip</li>
                        <li>Import Bigbox.ovf file in Virtualbox</li>
                    </ul>
                </section>

                <section>
                    <h3>Data download</h3>
                    <pre><code class="shell">$ cd ~/work/</code></pre>
                    <pre><code class="shell">$ wget http://classics.mit.edu/Homer/iliad.mb.txt
$ wget http://classics.mit.edu/Homer/odyssey.mb.txt</code></pre>
                </section>

                <section>
                    <h3>Python Shell</h3>
                    <pre><code class="shell">$ cd ~/work/spark-2.2.0-bin-hadoop2.7/</code></pre>
                    <pre><code class="shell">$ ./bin/pyspark
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
>>></code></pre>
                    <div class="fragment"><h3 class="orange">Python 3 Shell</h3>
                    <pre><code class="shell">$ PYSPARK_PYTHON=python3 ./bin/pyspark
Python 3.5.2 (default, Sep 14 2017, 22:51:06)
>>></code></pre></div>
                </section>
                <section>
                    <div><h3>iPython Shell</h3>
                    <pre><code class="shell">$ pip install --user ipython==5.5.0</code></pre>
                    <pre><code class="shell">$ PYSPARK_PYTHON=ipython ./bin/pyspark
Python 2.7.12 (default, Nov 19 2016, 06:48:10)
In [1]: </code></pre></div>
                    <div><h3>iPython 3 Shell</h3>
                    <pre><code class="shell">$ pip3 install --user ipython</code></pre>
                    <pre><code class="shell">$ PYSPARK_PYTHON=ipython3 ./bin/pyspark
Python 3.5.2 (default, Sep 14 2017, 22:51:06)
In [1]: </code></pre></div>
                </section>


                <section>
                    <h3>Your first resilient distributed dataset (RDD)</h3>
                    <pre class="fragment"><code class="python">>>> rdd = sc.parallelize(range(0, 10))</code></pre>
                    <pre class="fragment"><code class="python">>>> rdd.collect()
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</code></pre>
                    <pre class="fragment"><code class="python">>>> rdd.count()
10</code></pre>
                    <pre class="fragment"><code class="python">>>> rdd.first()
0</code></pre>
                    <pre class="fragment"><code class="python">>>> rdd.map(lambda x: x*x).collect()
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]</code></pre>
                </section>

                <section>
                    <h2>Wordcount! \o/</h2>
                    <pre class="fragment"><code class="python">>>> rdd = sc.textFile("iliad.mb.txt")</code></pre>
                    <pre class="fragment"><code class="python">>>> rdd.flatMap(lambda sentence: sentence.split())\
    .map(lambda word: (word, 1))\
    .reduceByKey(lambda v1, v2: v1 + v2)\
    .sortBy(lambda wc: -wc[1])\
    .take(10)</code></pre>
                    <pre class="fragment"><code class="python">[('the', 9573), ('and', 6481), ('of', 5584), ('to', 3291), ('his', 2487), ('he', 2448), ('in', 2184), ('a', 1789), ('with', 1593), ('that', 1434)]</code></pre>
                    <!--<pre>>>> rdd.count()-->
<!--13444</pre>-->
                    <!--<pre>>>> rdd.first()-->
<!--'Provided by The Internet Classics Archive.'</pre>-->
                    <!--<pre>>>> rdd.flatMap(lambda sentence: sentence.split()).first()-->
<!--'Provided'</pre>-->
                </section>

                <section>
                    <h3>Documentation</h3>
                    <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a>
                </section>

                <section>
                    <h3>RDD operations</h3>
                    <table>
                        <tr class="orange">
                            <th>Transformations</th>
                            <th>Actions</th>
                        </tr>
                        <tr class="fragment">
                            <td>map, distinct, filter, reduceByKey, sortByKey, join...</td>
                            <td>reduce, collect, count, first, take...</td>
                        </tr>
                        <tr class="fragment">
                            <td colspan="2">Arguments: 1 or more RDD</td>
                        </tr>
                        <tr class="fragment">
                            <td>Returns: RDD</td>
                            <td>Returns: not an RDD </td>
                        </tr>
                        <tr class="fragment">
                            <td>Lazy evaluation</td>
                            <td>Immediate evaluation</td>
                        </tr>
                        <tr class="fragment">
                            <td>Sometimes shuffle</td>
                            <td>Shuffle necessary</td>
                        </tr>
                    </table>
                </section>

                <section>
                    <h3>Directed Acyclic Graph (DAG)</h3>
                    <img src="static/img/dag.svg" alt="dag">
                </section>

                <section>
                    <h3>Running a script</h3>
                    <pre><code class="shell">$ vim wordcount.py</code></pre>
                    <pre><code class="python">from pyspark import SparkContext

sc = SparkContext()
rdd = sc.textFile("iliad.mb.txt")
result = rdd.flatMap(lambda sentence: sentence.split())\
    .map(lambda word: (word, 1))\
    .reduceByKey(lambda v1, v2: v1 + v2)\
    .sortBy(lambda wc: -wc[1])\
    .take(10)

print(result)
</code></pre>
                    <pre><code class="shell">$ ./spark-2.2.0-bin-hadoop2.7/bin/spark-submit ./wordcount.py</code></pre>
                </section>
                <section>
                    <h3>Advanced: Debugging with Spark UI</h3>
                    <p><a href="http://localhost:4040">http://localhost:4040</a></p>
                    <img src="static/img/sparkui.png" alt="Spark UI">

                    <div class="fragment">
                        <p>Pro tip: append this to your script</p>
                        <pre class="fragment"><code class="python">input("Access http://localhost:4040 to debug. Then press ctrl+c to exit")</code></pre>
                    </div>
                </section>

                <section>
                    <h3>Pro tip: reduce Spark logging level</h3>
                    <pre><code class="shell">$ cd spark-2.2.0-bin-hadoop2.7/conf/
$ cp log4j.properties.template log4j.properties
$ vim log4j.properties
...
log4j.rootCategory=ERROR, console
...</code></pre>
                </section>

                <section>
                    <h3>TODO (starter)</h3>
                    <ol>
                        <li>Print the top 10 most frequent words with their probability of appearance</li>
                        <li>Get rid of special characters (.,:!?')</li>
                        <li>Identify the transformations and the actions in your script</li>
                        <li>How many times are the transformations evaluated? (Hint: it depends)</li>
                        <li>Can you reduce this number? (Hint: check out "persist")</li>
                    </ol>
                </section>
                <section>
                    <h3>TODO (intermediate)</h3>
                    <ol>
                        <li>Print the top 10 words from the <i>Iliad</i> that have "most disappeared" in <i>The Odyssey</i> (Hint: you need to understand "join")</li>
                        <li>Do the same by swapping the <i>Iliad</i> and <i>The Odyssey</i></li>
                        <li>Improve your script by getting rid of stopwords:
                            <pre><code class="python">from nltk.corpus import stopwords
english_stop_words = stopwords.words("english")</code></pre>
                        </li>
                        <li>Use the Spark UI (http://localhost:4040) to make your script faster</li>
                    </ol>
                </section>

                <!--Part 2-->
                <section>
                    <h2>Calculs Distribués avec Apache Spark</h2>
                    <h4>Architectures distribuées et machine learning</h4>
                    <p><a href="https://minutebutterfly.com">Régis Behmo</a><br>
                    <a href="https://github.com/regisb"><i class="fa fa-github"></i></a>
                    <a href="https://stackoverflow.com/users/356528/r%c3%a9gis-b?tab=profile"><i class="fa fa-stack-overflow"></i></a>
                    regisb<br></p>
                    <p>CentraleSupelec, 20 novembre 2017</p>
                    <p>Slides: <a href="https://regisb.github.io/centralesupelec-2017">https://regisb.github.io/centralesupelec-2017</a></p>
                </section>
                <section>
                    <h3>Part 1 homework</h3>
                    <p><a href="https://github.com/regisb/centralesupelec-2017/blob/master/homework/iliad_odyssey.py">https://github.com/regisb/centralesupelec-2017/blob/master/homework/iliad_odyssey.py</a></p>
                </section>
                <section data-background-image="static/img/distributed-simple.svg" data-background-size="contain"></section>
                <section>
                    <h3>Configuration: launch Spark cluster</h3>
                    <p>Launch one master:</p>
                    <pre><code class="shell">./sbin/start-master.sh --host 192.168.1.<span class="orange">M</span> --port 7077</code></pre>
                    <p>Launch multiple slaves:</p>
                    <pre><code class="shell">./sbin/start-slave.sh --host 0.0.0.0 spark://192.168.1.<span class="orange">M</span>:7077</code></pre>
                    <p>(prefix with <code>SPARK_NO_DAEMONIZE=1</code> to launch workers in the foreground)
                </section>
                <section>
                    <h3>Spark shell</h3>
                    <pre><code class="shell">./bin/pyspark --master spark://192.168.1.<span class="orange">M</span>:7077</code></pre>
                </section>
                <section>
                    <h3>Launching jobs</h3>
                    <pre><code class="shell">./bin/spark-submit --master spark://192.168.1.<span class="orange">M</span>:7077 myscript.py</code></pre>
                </section>
                <section>
                    <h3>Configuration: resource allocation</h3>
                    <p>Slave:</p>
                    <pre><code class="shell">./sbin/start-slave.sh --host 0.0.0.0 <span class="orange">--cores 2 --memory 512m</span> spark://192.168.1.<span class="orange">M</span>:7077</code></pre>
                </section>
                <section>
                    <h3>Configuration: resource allocation</h3>
                    <p>Spark shell:</p>
                    <pre><code class="shell">./bin/pyspark --master spark://192.168.1.M:7077 \
    <span class="orange">--total-executor-cores 2 --executor-memory 512m</span></code></pre>
                    <p>Spark script:</p>
                    <pre><code class="shell">./bin/spark-submit --master spark://192.168.1.M:7077 \
    <span class="orange">--total-executor-cores 2 --executor-memory 512m</span> myscript.py</code></pre>
                </section>
                <section>
                    <h3>Configuration: resource allocation</h3>
                    <p>Application (optional):</p>
                    <pre><code class="python">from pyspark import SparkConf
conf = SparkConf()
conf.set("spark.cores.max", 2)
conf.set("spark.executor.memory", "512m")
sc = SparkContext(conf=conf)</code></pre>
                </section>
                <section>
                    <h3>Data loading</h3>
                    <pre><code class="python">sc.textFile("/home/student/work/iliad.mb.txt")</code></pre>
                    <pre class="fragment"><code class="python">sc.textFile("hdfs://192.168.1.101:9000/data/iliad.mb.txt")
sc.textFile("hdfs://192.168.1.101:9000/data/blogs/raw.txt")
sc.textFile("hdfs://192.168.1.101:9000/data/blogs/raw1000.txt")</code></pre>
                </section>
                <section>
                    <h3>Data loading</h3>
                    <pre><code class="shell">$ ./bin/hdfs dfs -fs hdfs://192.168.1.101:9000 -ls /data
drwxr-xr-x   - regis supergroup          0 2017-11-18 20:44 /data/blogs
-rw-r--r--   3 regis supergroup  691239883 2017-11-19 09:11 /data/enwik9-text
-rw-r--r--   3 regis supergroup     808298 2017-11-18 22:28 /data/iliad.mb.txt</code></pre>
                    <pre><code class="shell">$ ./bin/hdfs dfs -fs hdfs://192.168.1.101.1:9000 -copyToLocal /data/enwik9-text .</code></pre>
                </section>
                <section>
                    <h3>Spark cluster</h3>
                    <img src="static/img/distributed-simple.svg">
                </section>
                <section>
                    <h3>HDFS cluster</h3>
                    <img src="static/img/distributed-simple-hdfs.svg">
                </section>
                <section data-background-image="static/img/spark-hdfs.svg" data-background-size="contain"></section>
                <section>
                    <h3>Configuration: HDFS</h3>
                    <p>Launch one namenode:</p>
                    <pre><code class="shell">./bin/hdfs namenode -fs hdfs://192.168.1.<span class="orange">N</span>:9000</code></pre>
                    <p>Launch multiple datanodes:</p>
                    <pre><code class="shell">./bin/hdfs datanode -fs hdfs://192.168.1.<span class="orange">N</span>:9000</code></pre>
                </section>
                <section>
                    <h3>Local configuration</h3>
                    <p>Wifi SSID: "Spark CentraleSupelec" (No password)</p>
                    <p>Obtain your IP address: <code>ifconfig</code> (I'm at 192.168.1.101)</p>
                    <p>Virtualbox:</p>
                    <ul>
                        <li>Adjust your RAM/CPU</li>
                        <li>Check network settings: "Bridged Adapter" &#8594; "wlan0"</li>
                    </ul>
                </section>
                <section>
                    <h3>DEBUG</h3>
                    <ul>
                        <li><a href="http://localhost:8080">http://localhost:8080</a> = Spark master UI</li>
                        <li><a href="http://localhost:4040">http://localhost:4040</a> = Spark application UI</li>
                        <li><a href="http://localhost:50070">http://localhost:50070</a> = HDFS namenode UI</li>
                    </ul>
                </section>
                <section>
                    <h3>TODO (warmup)</h3>
                    <ol>
                        <li>Create a cluster of 4-6 Spark nodes</li>
                        <li>If possible, launch a couple HDFS datanodes</li>
                        <li>Launch wordcount.py on <code>hdfs://192.168.1.101:9000/data/iliad.mb.txt</code></li>
                        <li>Launch wordcount.py on <code>hdfs://192.168.1.101:9000/data/blogs/raw.txt</code></li>
                        <li>Launch two jobs at the same time. Make them run at the same time (Hint: check resource allocation)</li>
                        <li>What happens when a Spark node is brutally shutdown?</li>
                    </ol>
                </section>
                <section>
                    <h3>TODO (intermediate)</h3>
                    <ol>
                        <li>What is Word2Vec?</li>
                        <li>Create a Word2Vec model of the Iliad (Hint: it's better to have each paragraph on a single line, see <code>hdfs://192.168.1.101:9000/data/iliad.oneline.txt</code>)</li>
                        <li>Who is Achilles + (Priam - Hector)?</li>
                    </ol>
                </section>

                <!--Part 3-->
                <section>
                    <h2>Calculs Distribués avec Apache Spark</h2>
                    <h4>Traitement de données temps réel</h4>
                    <p><a href="https://minutebutterfly.com">Régis Behmo</a><br>
                    <a href="https://github.com/regisb"><i class="fa fa-github"></i></a>
                    <a href="https://stackoverflow.com/users/356528/r%c3%a9gis-b?tab=profile"><i class="fa fa-stack-overflow"></i></a>
                    regisb<br></p>
                    <p>CentraleSupelec, 27 novembre 2017</p>
                    <p>Slides: <a href="https://regisb.github.io/centralesupelec-2017">https://regisb.github.io/centralesupelec-2017</a></p>
                </section>
                <section>
                    <h3>Part 2 homework</h3>
                    <p><a href="https://github.com/regisb/centralesupelec-2017/blob/master/homework/word2vec.py">https://github.com/regisb/centralesupelec-2017/blob/master/homework/word2vec.py</a></p>
                </section>
                <section>
                    <pre><code class="python">from pyspark import SparkContext
from pyspark.mllib.feature import Word2Vec

sc = SparkContext()
rdd = sc.textFile("hdfs://192.168.1.101:9000/data/iliad.oneline.txt")\
    .map(lambda line: line.strip().split())\
    .map(lambda words: [w.strip(",.:;'\"-?!") for w in words])\
    .map(lambda words: [w for w in words if w]).filter(lambda s: s)
model = Word2Vec().setSeed(1).setVectorSize(200).fit(rdd)
vectors = model.getVectors()

def minus(vec1, vec2):
    return [v1 - v2 for v1, v2 in zip(vec1, vec2)]
def plus(vec1, vec2):
    return [v1 + v2 for v1, v2 in zip(vec1, vec2)]

synonyms = model.findSynonyms(plus(
    minus(vectors["Priam"], vectors["Hector"]),
    vectors["Achilles"]), 10)
print(list(synonyms))</code></pre>
                </section>
                <section>
                    <h3>Priam - Hector + Achilles</h3>
                    <p>(Peleus is to Achilles what Priam is to Hector)</p>
                    <pre><code>[('Priam', 0.94304829835891724),                                                
 ('Atreus', 0.88183891773223877),
 ('Saturn', 0.87939071655273438),
 <span class="orange">('Peleus', 0.87464809417724609)</span>,
 ('Telamon', 0.86552971601486206),
 ('Laertes', 0.8487703800201416),
 ('Nestor', 0.83410191535949707),
 ('noble', 0.81868565082550049),
 ('Agamemnon', 0.81820404529571533),
 ('Gerene', 0.81664806604385376)]</code></pre>
                </section>
                <section></section>
                <section>
                    <h3>Wordcount \o/ (streaming edition)</h3>
                    <p>Every 1s, print most frequent words</p>
                    <pre><code class="python">from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext()
ssc = StreamingContext(sc, 1)
ssc.checkpoint("./checkpoint")

def count_words(counts, current_count):
    if current_count is None:
        current_count = 0
    return current_count + sum(counts)

ssc.socketTextStream("localhost", 9999)\
    .flatMap(lambda line: line.split()).map(lambda word: (word, 1))\
    .updateStateByKey(count_words)\
    .transform(lambda rdd: rdd.sortBy(lambda wc: -wc[1]))\
    .foreachRDD(lambda rdd: print(rdd.take(10)))

ssc.start()
ssc.awaitTermination()</code></pre>
                </section>
                <section>
                    <h3>DStreams</h3>
                    <p>DStream = "Discretized stream" = Sequence of RDDs</p>
                    <img src="static/img/dstream.png">
                </section>
                <section>
                    <h3>DStreams</h3>
                    <p>DStream = "Discretized stream" = Sequence of RDDs</p>
                    <img src="static/img/dstream-lines.png">
                </section>
                <section>
                    <h3>Your first Spark Streaming app</h3>
                    <p>Make sure checkpoint directory exists</p>
                    <p>Launch convenient TCP server: <code>nc -lk 9999</code></p>
                </section>
                <section>
                    <h3>Windowing</h3>
                    <p>"Every 2s, aggregate data that arrived during the past 3s"</p>
                    <pre><code>.reduceByKeyAndWindow(func, func_inv, 3, 2)</code></pre>
                    <img src="static/img/windowing.png" style="background-color: white;">
                </section>
                <section>
                    <h3>Wordcount \o/ (sliding window edition)</h3>
                    <p>Every 1s, print words that were most frequent during the last 5s</p>
                    <pre><code class="python">from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext()
ssc = StreamingContext(sc, 1)
ssc.checkpoint("./checkpoint")

ssc.socketTextStream("localhost", 9999)\
    .flatMap(lambda line: line.split())\
    .map(lambda word: (word, 1))\
    .reduceByKeyAndWindow(lambda c1, c2: c1+c2, None, 5, 1)\
    .transform(lambda rdd: rdd.sortBy(lambda wc: -wc[1]))\
    .foreachRDD(lambda rdd: print(rdd.take(10)))

ssc.start()
ssc.awaitTermination()</code></pre>
                </section>
                <section>
                    <h3>Spark UI - streaming tab</h3>
                    <p><a href="http://localhost:4040/streaming/">http://localhost:4040/streaming/</a></p>
                    <img src="static/img/sparkui-streaming.png">
                </section>
                <section>
                    <h3>Bikes!</h3>
                    <p><a href="http://velib.behmo.com">http://velib.behmo.com</a></p>
                    <p>(data courtesy of <a href="https://developer.jcdecaux.com/">https://developer.jcdecaux.com/</a>)</p>
                    <pre>{
    "number":10120,
    <span class="orange">"name":"10120 - SALENGRO / DESCARTES"</span>,
    "address":"41 AV. ROGER SALENGRO (VILLEURBANNE)",
    "position":{"lat":45.7759505002626,"lng":4.87143421497628},
    "banking":true,
    "bonus":false,
    "status":"OPEN",
    <span class="orange">"contract_name":"Lyon"</span>,
    "bike_stands":15,
    "available_bike_stands":6,
    <span class="orange">"available_bikes":9</span>,
    <span class="orange">"last_update":1511735128000</span>
}</pre>
                </section>
                <section>
                    <h3>How to start...</h3>
                    <pre><code class="python">import json
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

sc = SparkContext()
ssc = StreamingContext(sc, 5)

stream = ssc.socketTextStream("velib.behmo.com", 9999)
stations = stream.map(lambda station: json.loads(station))\
    .map(lambda station: (
        station['contract_name'] + ' ' + station['name'],
        station['available_bikes']
    ))\
    .pprint()

ssc.checkpoint("./checkpoint")
ssc.start()
ssc.awaitTermination()
</code></pre>
                </section>
                <section>
                    <h3>TODO</h3>
                    <ol>
                        <li>Create a Spark Streaming app that reads Velib data from velib.behmo.com (port 9999)</li>
                        <li>Every 5s: print the empty Velib stations</li>
                        <li>Every 5s: print the Velib stations that have become empty</li>
                        <li>Every 1 min: print the stations that were most active during the last 5 min (activity = number of bikes borrowed and returned)</li>
                    </ol>
                    <p>Send your velib.py file to <a class="getintouch"></a> before Jan 8 2018</p>
                </section>

                <section>
                    <h3>Questions?</h3>
                    <p><i class="fa fa-envelope"></i> <a class="getintouch"></a></p>
                    <p>Slides: <a href="https://regisb.github.io/centralesupelec-2017">https://regisb.github.io/centralesupelec-2017</a></p>

                    <h3>Going further</h3>
                    <p><i class="fa fa-graduation-cap"></i> <a href="https://openclassrooms.com/courses/realisez-des-calculs-distribues-sur-des-donnees-massives">Cours Openclassrooms :</a> <i>Réalisez des calculs distribués sur des données massives</i> (Behmo &amp; Hudelot)</p>
                    <p><i class="fa fa-video-camera"></i> Videos : <a href="https://dataarchitect.minutebutterfly.com/">https://dataarchitect.minutebutterfly.com/</a></p>
                </section>
                <!--
                    - Archi d'une solution temps réel
                    - Windowing
                    - updateStateByKey
                    - Checkpointing
                    - Monitoring avec Spark UI
                    - Performance tuning
                    - Adjust batch size
                -->
            </div>
        </div>

        <script src="static/js/reveal.js"></script>
        <script src="static/js/jquery-3.2.1.min.js"></script>
        <script src="static/js/highlight.min.js"></script>
        <script>
            Reveal.initialize({
                controls: false,
                history: true,
                transition: 'fade'
            });
        </script>
        <script>
            $(document).ready(function() {
                var email = "sparkisfantastic";
                email += "@";
                email += "behmo.com";
                var addr = "mail";
                addr += "to";
                addr += ":";
                addr += email;
                addr += "?Subject=Help!";
                $(".getintouch").prop("href", encodeURI(addr));
                $(".getintouch").text(email);
            });
        </script>
        <script>hljs.initHighlightingOnLoad();</script>
    </body>
</html>
